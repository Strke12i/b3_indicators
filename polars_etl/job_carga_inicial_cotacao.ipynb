{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "import os\n",
    "import tempfile\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Suppress insecure request warnings\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações de banco de dados\n",
    "DATABASE_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"meu_banco\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin_password\",\n",
    "    \"port\": 5432\n",
    "}\n",
    "\n",
    "# --- Definições de colunas, mercados, etc. ---\n",
    "FIELD_SIZES = {\n",
    "    'TIPO_DE_REGISTRO': 2, 'DATA_DO_PREGAO': 8, 'CODIGO_BDI': 2, 'CODIGO_DE_NEGOCIACAO': 12,\n",
    "    'TIPO_DE_MERCADO': 3, 'NOME_DA_EMPRESA': 12, 'ESPECIFICACAO_DO_PAPEL': 10,\n",
    "    'PRAZO_EM_DIAS_DO_MERCADO_A_TERMO': 3, 'MOEDA_DE_REFERENCIA': 4, 'PRECO_DE_ABERTURA': 13,\n",
    "    'PRECO_MAXIMO': 13, 'PRECO_MINIMO': 13, 'PRECO_MEDIO': 13, 'PRECO_ULTIMO_NEGOCIO': 13,\n",
    "    'PRECO_MELHOR_OFERTA_DE_COMPRA': 13, 'PRECO_MELHOR_OFERTA_DE_VENDAS': 13,\n",
    "    'NUMERO_DE_NEGOCIOS': 5, 'QUANTIDADE_NEGOCIADA': 18, 'VOLUME_TOTAL_NEGOCIADO': 18,\n",
    "    'PRECO_DE_EXERCICIO': 13, 'INDICADOR_DE_CORRECAO_DE_PRECOS': 1, 'DATA_DE_VENCIMENTO': 8,\n",
    "    'FATOR_DE_COTACAO': 7, 'PRECO_DE_EXERCICIO_EM_PONTOS': 13, 'CODIGO_ISIN': 12,\n",
    "    'NUMERO_DE_DISTRIBUICAO': 3\n",
    "}\n",
    "\n",
    "FLOAT_COLUMNS = ['PRECO_DE_ABERTURA', 'PRECO_MAXIMO', 'PRECO_MINIMO', 'PRECO_MEDIO',\n",
    "                'PRECO_ULTIMO_NEGOCIO', 'PRECO_MELHOR_OFERTA_DE_COMPRA',\n",
    "                'PRECO_MELHOR_OFERTA_DE_VENDAS', 'PRECO_DE_EXERCICIO',\n",
    "                'PRECO_DE_EXERCICIO_EM_PONTOS', 'VOLUME_TOTAL_NEGOCIADO', 'QUANTIDADE_NEGOCIADA']\n",
    "\n",
    "DATE_COLUMNS = [\"DATA_DO_PREGAO\", \"DATA_DE_VENCIMENTO\"]\n",
    "\n",
    "CODBDI = {\n",
    "    '02': \"LOTE_PADRAO\"\n",
    "    # Simplificado para o que realmente usamos no filtro\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://bvmf.bmfbovespa.com.br/InstDados/SerHist/COTAHIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para inserir dados no banco\n",
    "def insert_into_database(df, batch_size=10000):\n",
    "    \"\"\"Insere os dados processados no banco de dados usando psycopg2 com execute_values.\"\"\"\n",
    "    if df.is_empty():\n",
    "        print(\"DataFrame vazio, nada a inserir.\")\n",
    "        return 0\n",
    "\n",
    "    conn = None\n",
    "    inserted = 0\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DATABASE_CONFIG)\n",
    "        with conn.cursor() as cursor:\n",
    "            # Preparar dados para inserção\n",
    "            data = df.to_pandas().to_dict('records')\n",
    "\n",
    "            # Inserir em lotes para melhor performance\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                values = [(\n",
    "                    row['codigo_isin'],\n",
    "                    row['data_pregao'],\n",
    "                    row['abertura'],\n",
    "                    row['fechamento'],\n",
    "                    row['numero_de_negocios'],\n",
    "                    row['quantidade_negociada'],\n",
    "                    row['volume_negociado']\n",
    "                ) for row in batch]\n",
    "\n",
    "                # Usar execute_values para inserção em lote\n",
    "                execute_values(\n",
    "                    cursor,\n",
    "                    \"\"\"\n",
    "                    INSERT INTO cotacao\n",
    "                    (codigo_isin, data_pregao, abertura, fechamento, numero_de_negocios,\n",
    "                     quantidade_negociada, volume_negociado)\n",
    "                    VALUES %s\n",
    "                    ON CONFLICT (codigo_isin, data_pregao) DO NOTHING\n",
    "                    \"\"\",\n",
    "                    values\n",
    "                )\n",
    "\n",
    "            conn.commit()\n",
    "            inserted = len(data)\n",
    "            print(f\"Inseridos/Ignorados ~{inserted} registros na tabela cotacao.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print(f\"Erro ao inserir no banco de dados: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "    return inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para obter tickers existentes no banco\n",
    "def get_existing_tickers():\n",
    "    \"\"\"Busca os tickers existentes no banco de dados.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DATABASE_CONFIG)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT codigo_isin FROM ticker\")\n",
    "            return set(row[0] for row in cursor.fetchall())\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao buscar tickers: {e}\")\n",
    "        return set()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para processar o arquivo de cotações\n",
    "def process_file(file_path, existing_tickers):\n",
    "    \"\"\"Processa o arquivo de cotações usando Polars.\"\"\"\n",
    "    print(f\"Processando arquivo: {file_path}\")\n",
    "\n",
    "    # Definir esquema para leitura mais eficiente\n",
    "    schema = {\n",
    "        'TIPO_DE_REGISTRO': pl.Utf8,\n",
    "        'DATA_DO_PREGAO': pl.Utf8,\n",
    "        'CODIGO_BDI': pl.Utf8,\n",
    "        'CODIGO_ISIN': pl.Utf8,\n",
    "        'PRECO_DE_ABERTURA': pl.Utf8,\n",
    "        'PRECO_ULTIMO_NEGOCIO': pl.Utf8,\n",
    "        'NUMERO_DE_NEGOCIOS': pl.Utf8,\n",
    "        'QUANTIDADE_NEGOCIADA': pl.Utf8,\n",
    "        'VOLUME_TOTAL_NEGOCIADO': pl.Utf8\n",
    "    }\n",
    "\n",
    "    # Criar uma lista de posições de coluna baseada no FIELD_SIZES\n",
    "    column_positions = []\n",
    "    current_pos = 0\n",
    "    for col, width in FIELD_SIZES.items():\n",
    "        column_positions.append((current_pos, current_pos + width))\n",
    "        current_pos += width\n",
    "\n",
    "    # Filtrar apenas as colunas que precisamos\n",
    "    needed_columns = ['TIPO_DE_REGISTRO', 'DATA_DO_PREGAO', 'CODIGO_BDI', 'PRECO_DE_ABERTURA',\n",
    "                      'PRECO_ULTIMO_NEGOCIO', 'NUMERO_DE_NEGOCIOS', 'QUANTIDADE_NEGOCIADA',\n",
    "                      'VOLUME_TOTAL_NEGOCIADO', 'CODIGO_ISIN']\n",
    "\n",
    "    needed_positions = []\n",
    "    column_names = []\n",
    "    for i, col in enumerate(FIELD_SIZES.keys()):\n",
    "        if col in needed_columns:\n",
    "            needed_positions.append(column_positions[i])\n",
    "            column_names.append(col)\n",
    "\n",
    "    # Ler o arquivo usando Polars com leitura por colunas fixas\n",
    "    # Pular a primeira linha (cabeçalho) e usar LazyFrame para processamento eficiente\n",
    "    df = pl.read_csv(\n",
    "        file_path,\n",
    "        has_header=False,\n",
    "        skip_rows=1,\n",
    "        encoding='latin1',\n",
    "        separator='\\n',  # Cada linha é um registro\n",
    "        columns=[0],     # Lê apenas a primeira coluna que contém toda a linha\n",
    "        new_columns=[\"line\"]  # Renomeia para \"line\"\n",
    "    ).lazy()\n",
    "\n",
    "    # Extrair as colunas necessárias da linha usando expressões\n",
    "    for i, (col_name, (start, end)) in enumerate(zip(column_names, needed_positions)):\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"line\").str.slice(start, end - start).alias(col_name)\n",
    "        ])\n",
    "\n",
    "    # Filtrar linhas que não são trailer (TIPO_DE_REGISTRO != '99')\n",
    "    df = df.filter(pl.col(\"TIPO_DE_REGISTRO\") != \"99\")\n",
    "\n",
    "    # Filtrar por CODIGO_BDI == \"02\" (LOTE_PADRAO)\n",
    "    df = df.filter(pl.col(\"CODIGO_BDI\") == \"02\")\n",
    "\n",
    "    # Converter tipos de dados\n",
    "    df = df.with_columns([\n",
    "        # Converter datas\n",
    "        pl.col(\"DATA_DO_PREGAO\").str.to_date(\"%Y%m%d\").alias(\"data_pregao\"),\n",
    "\n",
    "        # Converter valores numéricos\n",
    "        (pl.col(\"PRECO_DE_ABERTURA\").cast(pl.Float64) / 100).alias(\"abertura\"),\n",
    "        (pl.col(\"PRECO_ULTIMO_NEGOCIO\").cast(pl.Float64) / 100).alias(\"fechamento\"),\n",
    "        pl.col(\"NUMERO_DE_NEGOCIOS\").cast(pl.Int64).alias(\"numero_de_negocios\"),\n",
    "        pl.col(\"QUANTIDADE_NEGOCIADA\").cast(pl.Float64).alias(\"quantidade_negociada\"),\n",
    "        pl.col(\"VOLUME_TOTAL_NEGOCIADO\").cast(pl.Float64).alias(\"volume_negociado\"),\n",
    "\n",
    "        # Manter CODIGO_ISIN como está\n",
    "        pl.col(\"CODIGO_ISIN\").alias(\"codigo_isin\")\n",
    "    ])\n",
    "\n",
    "    # Selecionar apenas as colunas finais e filtrar por tickers existentes\n",
    "    df = df.select([\n",
    "        \"codigo_isin\", \"data_pregao\", \"abertura\", \"fechamento\",\n",
    "        \"numero_de_negocios\", \"quantidade_negociada\", \"volume_negociado\"\n",
    "    ]).filter(\n",
    "        pl.col(\"codigo_isin\").is_in(list(existing_tickers))\n",
    "    )\n",
    "\n",
    "    # Executar o processamento e retornar o DataFrame materializado\n",
    "    return df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url_suffix):\n",
    "    \"\"\"Baixa e extrai o arquivo ZIP da B3.\"\"\"\n",
    "    url = f\"{BASE_URL}{url_suffix}\"\n",
    "    tmp_zip_path = None\n",
    "    tmp_txt_path = None\n",
    "\n",
    "    try:\n",
    "        print(f\"Baixando {url}...\")\n",
    "        response = requests.get(url, verify=False, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Salvar ZIP em arquivo temporário\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp_zip_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192 * 16):\n",
    "                tmp_zip_file.write(chunk)\n",
    "            tmp_zip_path = tmp_zip_file.name\n",
    "\n",
    "        print(f\"Arquivo ZIP salvo em: {tmp_zip_path}\")\n",
    "\n",
    "        # Criar um arquivo temporário para o TXT\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\") as tmp_txt_file:\n",
    "            tmp_txt_path = tmp_txt_file.name\n",
    "\n",
    "        # Fechar e remover o arquivo temporário vazio para evitar conflitos\n",
    "        os.remove(tmp_txt_path)\n",
    "\n",
    "        # Extrair TXT do ZIP\n",
    "        file_name_in_zip = url_suffix.replace('.ZIP', '.TXT')\n",
    "        file_name_in_zip = \"COTAHIST\" + file_name_in_zip\n",
    "\n",
    "        print(f\"Procurando arquivo {file_name_in_zip} no ZIP...\")\n",
    "\n",
    "        with zipfile.ZipFile(tmp_zip_path, 'r') as zf:\n",
    "            if file_name_in_zip not in zf.namelist():\n",
    "                print(f\"ERRO: Arquivo {file_name_in_zip} não encontrado no ZIP\")\n",
    "                print(f\"Arquivos no ZIP: {zf.namelist()}\")\n",
    "                return None\n",
    "\n",
    "            print(f\"Extraindo {file_name_in_zip} para {tmp_txt_path}...\")\n",
    "\n",
    "            # Extrair diretamente para o caminho temporário\n",
    "            with zf.open(file_name_in_zip) as source, open(tmp_txt_path, 'wb') as target:\n",
    "                target.write(source.read())\n",
    "\n",
    "        print(f\"Arquivo extraído com sucesso para: {tmp_txt_path}\")\n",
    "        return tmp_txt_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar/extrair arquivo {url}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        # Limpar arquivo ZIP temporário\n",
    "        if tmp_zip_path and os.path.exists(tmp_zip_path):\n",
    "            os.remove(tmp_zip_path)\n",
    "            print(f\"Arquivo ZIP temporário removido: {tmp_zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções para gerar sufixos de arquivos\n",
    "def get_years_range():\n",
    "    current_year = datetime.now().year\n",
    "    return range(2024, current_year + 1)\n",
    "\n",
    "def get_months_current_year():\n",
    "    current_date = datetime.now()\n",
    "    current_month = current_date.month\n",
    "    current_year = current_date.year\n",
    "    return [f\"_M{month:02d}{current_year}.ZIP\" for month in range(1, current_month)]\n",
    "\n",
    "def get_days_current_month():\n",
    "    current_date = datetime.now()\n",
    "    current_year = current_date.year\n",
    "    current_month = current_date.month\n",
    "    current_day = current_date.day\n",
    "    if current_day <= 1:\n",
    "        return []\n",
    "    return [f\"_D{day:02d}{current_month:02d}{current_year}.ZIP\" for day in range(1, current_day)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal de ETL\n",
    "def run_etl():\n",
    "    \"\"\"Função principal que executa o ETL completo.\"\"\"\n",
    "    print(\"Iniciando ETL de cotações B3...\")\n",
    "\n",
    "    # Obter tickers existentes\n",
    "    existing_tickers = get_existing_tickers()\n",
    "    print(f\"Encontrados {len(existing_tickers)} tickers no banco de dados.\")\n",
    "\n",
    "    if not existing_tickers:\n",
    "        print(\"AVISO: Nenhum ticker encontrado. Nenhuma cotação será inserida.\")\n",
    "        return\n",
    "\n",
    "    # Processar arquivos anuais\n",
    "    print(\"\\n=== Processando arquivos anuais ===\")\n",
    "    for year in get_years_range():\n",
    "        url_suffix = f\"_A{year}.ZIP\"\n",
    "        print(f\"Processando ano {year}...\")\n",
    "\n",
    "        # Download e extração\n",
    "        txt_path = download_and_extract(url_suffix)\n",
    "        if not txt_path:\n",
    "            print(f\"Falha ao baixar/extrair arquivo do ano {year}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Processar arquivo\n",
    "            df = process_file(txt_path, existing_tickers)\n",
    "\n",
    "            # Inserir no banco\n",
    "            if not df.is_empty():\n",
    "                inserted = insert_into_database(df)\n",
    "                print(f\"Processamento do ano {year} concluído. Registros: {inserted}\")\n",
    "            else:\n",
    "                print(f\"Nenhum registro válido encontrado para o ano {year}\")\n",
    "        finally:\n",
    "            # Limpar arquivo temporário\n",
    "            if os.path.exists(txt_path):\n",
    "                os.remove(txt_path)\n",
    "\n",
    "    # Processar arquivos mensais\n",
    "    print(\"\\n=== Processando arquivos mensais ===\")\n",
    "    for month_suffix in get_months_current_year():\n",
    "        print(f\"Processando mês {month_suffix}...\")\n",
    "\n",
    "        # Download e extração\n",
    "        txt_path = download_and_extract(month_suffix)\n",
    "        if not txt_path:\n",
    "            print(f\"Falha ao baixar/extrair arquivo do mês {month_suffix}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Processar arquivo\n",
    "            df = process_file(txt_path, existing_tickers)\n",
    "\n",
    "            # Inserir no banco\n",
    "            if not df.is_empty():\n",
    "                inserted = insert_into_database(df)\n",
    "                print(f\"Processamento do mês {month_suffix} concluído. Registros: {inserted}\")\n",
    "            else:\n",
    "                print(f\"Nenhum registro válido encontrado para o mês {month_suffix}\")\n",
    "        finally:\n",
    "            # Limpar arquivo temporário\n",
    "            if os.path.exists(txt_path):\n",
    "                os.remove(txt_path)\n",
    "\n",
    "    # Processar arquivos diários\n",
    "    print(\"\\n=== Processando arquivos diários ===\")\n",
    "    for day_suffix in get_days_current_month():\n",
    "        print(f\"Processando dia {day_suffix}...\")\n",
    "\n",
    "        # Download e extração\n",
    "        txt_path = download_and_extract(day_suffix)\n",
    "        if not txt_path:\n",
    "            print(f\"Falha ao baixar/extrair arquivo do dia {day_suffix}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Processar arquivo\n",
    "            df = process_file(txt_path, existing_tickers)\n",
    "\n",
    "            # Inserir no banco\n",
    "            if not df.is_empty():\n",
    "                inserted = insert_into_database(df)\n",
    "                print(f\"Processamento do dia {day_suffix} concluído. Registros: {inserted}\")\n",
    "            else:\n",
    "                print(f\"Nenhum registro válido encontrado para o dia {day_suffix}\")\n",
    "        finally:\n",
    "            # Limpar arquivo temporário\n",
    "            if os.path.exists(txt_path):\n",
    "                os.remove(txt_path)\n",
    "\n",
    "    print(\"\\nETL de cotações B3 concluído com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ETL de cotações B3...\n",
      "Encontrados 520 tickers no banco de dados.\n",
      "\n",
      "=== Processando arquivos anuais ===\n",
      "Processando ano 2024...\n",
      "Baixando https://bvmf.bmfbovespa.com.br/InstDados/SerHist/COTAHIST_A2024.ZIP...\n",
      "Arquivo ZIP salvo em: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpqp7ku3lf.zip\n",
      "Procurando arquivo COTAHIST_A2024.TXT no ZIP...\n",
      "Extraindo COTAHIST_A2024.TXT para C:\\Users\\felip\\AppData\\Local\\Temp\\tmpmf8ck2q6.txt...\n",
      "Arquivo extraído com sucesso para: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpmf8ck2q6.txt\n",
      "Arquivo ZIP temporário removido: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpqp7ku3lf.zip\n",
      "Processando arquivo: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpmf8ck2q6.txt\n",
      "Inseridos/Ignorados ~78057 registros na tabela cotacao.\n",
      "Processamento do ano 2024 concluído. Registros: 78057\n",
      "Processando ano 2025...\n",
      "Baixando https://bvmf.bmfbovespa.com.br/InstDados/SerHist/COTAHIST_A2025.ZIP...\n",
      "Arquivo ZIP salvo em: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpr1fwp4qp.zip\n",
      "Procurando arquivo COTAHIST_A2025.TXT no ZIP...\n",
      "Extraindo COTAHIST_A2025.TXT para C:\\Users\\felip\\AppData\\Local\\Temp\\tmpjgntdasz.txt...\n",
      "Arquivo extraído com sucesso para: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpjgntdasz.txt\n",
      "Arquivo ZIP temporário removido: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpr1fwp4qp.zip\n",
      "Processando arquivo: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpjgntdasz.txt\n",
      "Inseridos/Ignorados ~26024 registros na tabela cotacao.\n",
      "Processamento do ano 2025 concluído. Registros: 26024\n",
      "\n",
      "=== Processando arquivos mensais ===\n",
      "Processando mês _M012025.ZIP...\n",
      "Baixando https://bvmf.bmfbovespa.com.br/InstDados/SerHist/COTAHIST_M012025.ZIP...\n",
      "Arquivo ZIP salvo em: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpe9xfyi52.zip\n",
      "Procurando arquivo COTAHIST_M012025.TXT no ZIP...\n",
      "Extraindo COTAHIST_M012025.TXT para C:\\Users\\felip\\AppData\\Local\\Temp\\tmpmho9_kyf.txt...\n",
      "Arquivo extraído com sucesso para: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpmho9_kyf.txt\n",
      "Arquivo ZIP temporário removido: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpe9xfyi52.zip\n",
      "Processando arquivo: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpmho9_kyf.txt\n",
      "Inseridos/Ignorados ~6920 registros na tabela cotacao.\n",
      "Processamento do mês _M012025.ZIP concluído. Registros: 6920\n",
      "Processando mês _M022025.ZIP...\n",
      "Baixando https://bvmf.bmfbovespa.com.br/InstDados/SerHist/COTAHIST_M022025.ZIP...\n",
      "Arquivo ZIP salvo em: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpor_4wens.zip\n",
      "Procurando arquivo COTAHIST_M022025.TXT no ZIP...\n",
      "Extraindo COTAHIST_M022025.TXT para C:\\Users\\felip\\AppData\\Local\\Temp\\tmp6md2ibu8.txt...\n",
      "Arquivo extraído com sucesso para: C:\\Users\\felip\\AppData\\Local\\Temp\\tmp6md2ibu8.txt\n",
      "Arquivo ZIP temporário removido: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpor_4wens.zip\n",
      "Processando arquivo: C:\\Users\\felip\\AppData\\Local\\Temp\\tmp6md2ibu8.txt\n",
      "Inseridos/Ignorados ~6305 registros na tabela cotacao.\n",
      "Processamento do mês _M022025.ZIP concluído. Registros: 6305\n",
      "Processando mês _M032025.ZIP...\n",
      "Baixando https://bvmf.bmfbovespa.com.br/InstDados/SerHist/COTAHIST_M032025.ZIP...\n",
      "Arquivo ZIP salvo em: C:\\Users\\felip\\AppData\\Local\\Temp\\tmplvarixmk.zip\n",
      "Procurando arquivo COTAHIST_M032025.TXT no ZIP...\n",
      "Extraindo COTAHIST_M032025.TXT para C:\\Users\\felip\\AppData\\Local\\Temp\\tmpo413vpb4.txt...\n",
      "Arquivo extraído com sucesso para: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpo413vpb4.txt\n",
      "Arquivo ZIP temporário removido: C:\\Users\\felip\\AppData\\Local\\Temp\\tmplvarixmk.zip\n",
      "Processando arquivo: C:\\Users\\felip\\AppData\\Local\\Temp\\tmpo413vpb4.txt\n",
      "Inseridos/Ignorados ~6048 registros na tabela cotacao.\n",
      "Processamento do mês _M032025.ZIP concluído. Registros: 6048\n",
      "Processando mês _M042025.ZIP...\n",
      "Baixando https://bvmf.bmfbovespa.com.br/InstDados/SerHist/COTAHIST_M042025.ZIP...\n"
     ]
    }
   ],
   "source": [
    "# Executar o ETL\n",
    "if __name__ == \"__main__\":\n",
    "    run_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
