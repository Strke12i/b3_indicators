{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import requests, datetime\n",
    "from io import BytesIO, StringIO\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData, Table, update\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_itr_data_by_year(year):\n",
    "    try:\n",
    "        \n",
    "        url = f\"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/ITR/DADOS/itr_cia_aberta_{year}.zip\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    \n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "            # Listar arquivos no ZIP (opcional)\n",
    "            print(\"Arquivos no ZIP:\", zip_file.namelist())\n",
    "            # BPA - Balanço Patrimonial Ativo\n",
    "            # BPP - Balanço Patrimonial Passivo\n",
    "            # DRA - Demonstração do Resultado \n",
    "            # DFC_MI - Demonstração do Fluxo de Caixa Método Indireto\n",
    "\n",
    "            df_bpa = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"itr_cia_aberta_BPA_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "            df_bpp = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"itr_cia_aberta_BPP_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "            df_dre = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"itr_cia_aberta_DRE_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "            df_dfc_mi = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"itr_cia_aberta_DFC_MI_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "\n",
    "        df_dfc_mi[\"GRUPO_DFP\"] = \"DF Consolidado - Demonstração do Fluxo de Caixa\"\n",
    "        return df_bpa, df_bpp, df_dre, df_dfc_mi\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfp_data_by_year(year):\n",
    "    try:\n",
    "        \n",
    "        url = f\"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/DFP/DADOS/dfp_cia_aberta_{year}.zip\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    \n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "            # Listar arquivos no ZIP (opcional)\n",
    "            #print(\"Arquivos no ZIP:\", zip_file.namelist())\n",
    "            # BPA - Balanço Patrimonial Ativo\n",
    "            # BPP - Balanço Patrimonial Passivo\n",
    "            # DRA - Demonstração do Resultado \n",
    "            # DFC_MI - Demonstração do Fluxo de Caixa Método Indireto\n",
    "\n",
    "            df_bpa = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"dfp_cia_aberta_BPA_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "            df_bpp = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"dfp_cia_aberta_BPP_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "            df_dre = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"dfp_cia_aberta_DRE_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "            df_dfc_mi = pd.read_csv(\n",
    "                StringIO(zip_file.read(f\"dfp_cia_aberta_DFC_MI_con_{year}.csv\").decode(\"latin1\")),\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "        \n",
    "        df_dfc_mi[\"GRUPO_DFP\"] = \"DF Consolidado - Demonstração do Fluxo de Caixa\"\n",
    "        return df_bpa, df_bpp, df_dre, df_dfc_mi\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_15772\\2985202177.py:5: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  metadata = MetaData(bind=engine)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine('postgresql+psycopg2://admin:admin_password@localhost:5432/meu_banco')\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "metadata = MetaData(bind=engine)\n",
    "\n",
    "empresa = Table('empresa', metadata, autoload_with=engine)\n",
    "relatorio = Table('relatorio', metadata, autoload_with=engine)\n",
    "dados_relatorio = Table('dados_relatorio', metadata, autoload_with=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_v2(df_data):\n",
    "    try:\n",
    "        try:\n",
    "            df_data['CD_CVM'] = df_data['CD_CVM'].astype(str).str.replace(',', '', regex=True).astype(float).astype(int)\n",
    "        except Exception as e:\n",
    "            print(\"Erro ao converter CD_CVM:\", e)\n",
    "            print(\"Valores problemáticos:\", df_data[~df_data['CD_CVM'].astype(str).str.replace(',', '', regex=True).str.isnumeric()])\n",
    "        \n",
    "        if 'DT_INI_EXERC' in df_data.columns:\n",
    "            df_data['DT_INI_EXERC'] = df_data['DT_INI_EXERC'].where(pd.notna(df_data['DT_INI_EXERC']), None)\n",
    "        else:\n",
    "            df_data['DT_INI_EXERC'] = None\n",
    "        \n",
    "        try:\n",
    "            df_data['CD_CONTA'] = df_data['CD_CONTA'].astype(str).str.replace('.', '').str.replace(',', '', regex=True).astype(int)\n",
    "        except Exception as e:\n",
    "            print(\"Erro ao converter CD_CONTA:\", e)\n",
    "            print(\"Valores problemáticos:\", df_data[~df_data['CD_CONTA'].astype(str).str.replace('.', '').str.replace(',', '', regex=True).str.isnumeric()])\n",
    "\n",
    "                \n",
    "        print(\"Inserindo dados para o relatório:\", df_data['GRUPO_DFP'].unique()[0])\n",
    "        \n",
    "        df_data['DT_FIM_EXERC'] = df_data['DT_FIM_EXERC'].where(pd.notna(df_data['DT_FIM_EXERC']), None)\n",
    "        df_data['VL_CONTA'] = df_data['VL_CONTA'].apply(lambda x: float(x) if pd.notna(x) else 0.0)\n",
    "        \n",
    "        unique_cvms = [int(x) for x in df_data['CD_CVM'].unique()]\n",
    "        \n",
    "        existing_empresas = {int(e.id_empresa) for e in session.query(empresa.c.id_empresa).filter(\n",
    "            empresa.c.id_empresa.in_(unique_cvms)).all()}\n",
    "        \n",
    "        empresas_to_insert = []\n",
    "        \n",
    "        for code_cvm in unique_cvms:\n",
    "            if int(code_cvm) not in existing_empresas:\n",
    "                try:\n",
    "                    empresas_to_insert.append({\n",
    "                        \"id_empresa\": int(code_cvm), \n",
    "                        \"nome_empresa\": str(df_data[df_data['CD_CVM'] == code_cvm].iloc[0]['DENOM_CIA'])\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao preparar empresa {code_cvm}: {e}\")\n",
    "        \n",
    "        if empresas_to_insert:\n",
    "            try:\n",
    "                from sqlalchemy.dialects.postgresql import insert\n",
    "                stmt = insert(empresa).values(empresas_to_insert)\n",
    "                stmt = stmt.on_conflict_do_nothing(index_elements=['id_empresa'])\n",
    "                session.execute(stmt)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                print(\"Erro ao inserir empresas:\", e)\n",
    "                session.rollback()  # Garante que o erro não afete as próximas operações\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erro no pré-processamento:\", e)\n",
    "        return  # Evita continuar se houver erro crítico\n",
    "\n",
    "    try:\n",
    "        relatorios_data = []\n",
    "        relatorios_keys = set()\n",
    "        \n",
    "        for code_cvm in unique_cvms:\n",
    "            df_group = df_data[df_data['CD_CVM'] == code_cvm]\n",
    "            nome_relatorio = str(df_group.iloc[0][\"GRUPO_DFP\"].split(\" - \")[1])\n",
    "            \n",
    "            group_columns = ['DT_FIM_EXERC'] if df_data['DT_INI_EXERC'].isnull().all() else ['DT_INI_EXERC', 'DT_FIM_EXERC']\n",
    "            date_combinations = df_group[group_columns].drop_duplicates()\n",
    "            \n",
    "            for _, date_row in date_combinations.iterrows():\n",
    "                data_inicio = date_row['DT_INI_EXERC'] if 'DT_INI_EXERC' in date_row else None\n",
    "                data_fim = date_row['DT_FIM_EXERC']\n",
    "                \n",
    "                key = (nome_relatorio, int(code_cvm), data_inicio, data_fim)\n",
    "                if key not in relatorios_keys:\n",
    "                    relatorios_data.append({\n",
    "                        \"tipo_relatorio\": nome_relatorio,\n",
    "                        \"id_empresa\": int(code_cvm),\n",
    "                        \"data_inicio\": data_inicio,\n",
    "                        \"data_fim\": data_fim\n",
    "                    })\n",
    "                    relatorios_keys.add(key)\n",
    "        \n",
    "        if relatorios_data:\n",
    "            try:\n",
    "                from sqlalchemy.dialects.postgresql import insert\n",
    "                stmt = insert(relatorio).values(relatorios_data)\n",
    "                stmt = stmt.on_conflict_do_nothing(index_elements=['tipo_relatorio', 'id_empresa', 'data_inicio', 'data_fim'])\n",
    "                session.execute(stmt)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                print(\"Erro ao inserir relatórios:\", e)\n",
    "                session.rollback()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao processar relatórios:\", e)\n",
    "\n",
    "    try:\n",
    "        inserted_relatorios = {}\n",
    "        for r in session.query(relatorio).filter(relatorio.c.id_empresa.in_(unique_cvms)).all():\n",
    "            data_inicio = r.data_inicio.strftime('%Y-%m-%d') if r.data_inicio else None\n",
    "            data_fim = r.data_fim.strftime('%Y-%m-%d') if r.data_fim else None\n",
    "            key = (r.tipo_relatorio, int(r.id_empresa), data_inicio, data_fim)\n",
    "            inserted_relatorios[key] = int(r.id_relatorio)\n",
    "\n",
    "        dados_relatorios_data = {}\n",
    "\n",
    "        for code_cvm in unique_cvms:\n",
    "            df_group = df_data[df_data['CD_CVM'] == code_cvm]\n",
    "            nome_relatorio = str(df_group.iloc[0][\"GRUPO_DFP\"].split(\" - \")[1])\n",
    "\n",
    "            group_columns = ['DT_FIM_EXERC'] if df_data['DT_INI_EXERC'].isnull().all() else ['DT_INI_EXERC', 'DT_FIM_EXERC']\n",
    "            grouped = df_group.groupby(group_columns)\n",
    "\n",
    "            for group_key, group in grouped:\n",
    "                if len(group_columns) == 1:\n",
    "                    data_fim = group_key\n",
    "                    data_inicio = None\n",
    "                else:\n",
    "                    data_inicio, data_fim = group_key\n",
    "\n",
    "                data_inicio = data_inicio[0] if isinstance(data_inicio, tuple) else data_inicio\n",
    "                data_fim = data_fim[0] if isinstance(data_fim, tuple) else data_fim\n",
    "\n",
    "                if isinstance(data_inicio, pd.Timestamp):\n",
    "                    data_inicio = data_inicio.strftime('%Y-%m-%d')\n",
    "\n",
    "                if isinstance(data_fim, pd.Timestamp):\n",
    "                    data_fim = data_fim.strftime('%Y-%m-%d')\n",
    "\n",
    "                relatorio_id = inserted_relatorios.get((nome_relatorio, int(code_cvm), data_inicio, data_fim))\n",
    "\n",
    "                if relatorio_id:\n",
    "                    for _, row in group.iterrows():\n",
    "                        descricao = str(row[\"DS_CONTA\"])\n",
    "                        valor = float(str(row[\"VL_CONTA\"]).replace(\",\", '.')) if pd.notna(row[\"VL_CONTA\"]) else 0.0\n",
    "                        codigo_conta = int(str(row[\"CD_CONTA\"]).replace(\".\", \"\"))\n",
    "\n",
    "                        key = (relatorio_id, codigo_conta)\n",
    "\n",
    "                        dados_relatorios_data[key] = {\n",
    "                            \"id_relatorio\": relatorio_id,\n",
    "                            \"codigo_conta\": codigo_conta,\n",
    "                            \"descricao\": descricao,\n",
    "                            \"valor\": valor\n",
    "                        }\n",
    "\n",
    "        if dados_relatorios_data:\n",
    "            try:\n",
    "                from sqlalchemy.dialects.postgresql import insert\n",
    "                dados_relatorios_data = list(dados_relatorios_data.values())\n",
    "                stmt = insert(dados_relatorio).values(dados_relatorios_data)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['id_relatorio', 'codigo_conta'],\n",
    "                    set_={'descricao': stmt.excluded.descricao, 'valor': stmt.excluded.valor}\n",
    "                )\n",
    "                session.execute(stmt)\n",
    "                session.commit()\n",
    "                print(\"Dados inseridos com sucesso!\")\n",
    "            except Exception as e:\n",
    "                print(\"Erro ao inserir dados:\", e)\n",
    "                session.rollback()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erro final:\", e)\n",
    "        session.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def buscar_datas(url):\n",
    "    \"\"\"\n",
    "    Faz scraping da página no 'url' e retorna\n",
    "    um dicionário no formato {nome_arquivo.zip: data_modificacao}.\n",
    "    Ex.: {\"itr_cia_aberta_2021.zip\": \"30-Mar-2025\", ...}\n",
    "    \"\"\"\n",
    "    response_text = requests.get(url).text\n",
    "    html_parsed = BeautifulSoup(response_text, \"html.parser\")\n",
    "\n",
    "    # Encontra o bloco <pre>, divide por linhas e filtra somente .zip\n",
    "    lines = html_parsed.find_all(\"pre\")[0].text.split(\"\\r\")\n",
    "    for line in lines[:]:\n",
    "        if \".zip\" not in line:\n",
    "            lines.remove(line)\n",
    "\n",
    "    lines = [l.replace(\"\\n\", \"\") for l in lines]\n",
    "\n",
    "    arquivos = {}\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 2:\n",
    "            nome_arquivo = parts[0]\n",
    "            data_mod = parts[1]\n",
    "            arquivos[nome_arquivo] = data_mod\n",
    "\n",
    "    return arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_itr = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/ITR/DADOS/\"\n",
    "url_dfp = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/DFP/DADOS/\"\n",
    "\n",
    "def _buscar_arquivos_modificados():\n",
    "    \"\"\"\n",
    "    Busca no site da CVM, via scraping, os arquivos (ITR e DFP) disponíveis.\n",
    "    Retorna um dicionário do tipo:\n",
    "    {\n",
    "        2011: {\"ITR\": <string data mod>, \"DFP\": <string data mod>},\n",
    "        2012: ...\n",
    "    }\n",
    "    SOMENTE para os arquivos cuja data_mod seja a data de HOJE.\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    # Faz a leitura de todos os .zip em cada URL\n",
    "    arquivos_itr = buscar_datas(url_itr)  # dict: {\"itr_cia_aberta_2021.zip\": \"30-Mar-2025\", ...}\n",
    "    arquivos_dfp = buscar_datas(url_dfp)  # dict: {\"dfp_cia_aberta_2021.zip\": \"30-Mar-2025\", ...}\n",
    "    # Data de hoje (no formato date, sem horas)\n",
    "    # Faz a data de hoje ser 06-Apr-2025\n",
    "    hoje = datetime.date(2025, 4, 6)\n",
    "    #hoje = datetime.datetime.today().date()\n",
    "    # Função auxiliar para verificar se a data_mod = HOJE\n",
    "    def data_e_hoje(data_mod_str):\n",
    "        # Exemplo de data_mod_str: \"05-Aug-2024\"\n",
    "        # Formato a ser parseado: '%d-%b-%Y'\n",
    "        # se houver variação, ajuste o formato.\n",
    "        try:\n",
    "            data_mod_date = datetime.datetime.strptime(data_mod_str, '%d-%b-%Y').date()\n",
    "            return data_mod_date == hoje\n",
    "        except ValueError:\n",
    "            return False\n",
    "    # Monta dicionário com anos que devem ser processados\n",
    "    anos_processar = {}\n",
    "    # Processa ITR\n",
    "    for nome_arquivo, data_mod in arquivos_itr.items():\n",
    "        if not data_e_hoje(data_mod):\n",
    "            # Se NÃO é hoje, pula\n",
    "            continue\n",
    "        try:\n",
    "            # \"itr_cia_aberta_2021.zip\" -> extrai \"2021\"\n",
    "            year = int(nome_arquivo.replace(\"itr_cia_aberta_\", \"\").replace(\".zip\", \"\"))\n",
    "        except:\n",
    "            continue\n",
    "        if year not in anos_processar:\n",
    "            anos_processar[year] = {\"ITR\": None, \"DFP\": None}\n",
    "        anos_processar[year][\"ITR\"] = data_mod\n",
    "    # Processa DFP\n",
    "    for nome_arquivo, data_mod in arquivos_dfp.items():\n",
    "        if not data_e_hoje(data_mod):\n",
    "            # Se NÃO é hoje, pula\n",
    "            continue\n",
    "        try:\n",
    "            # \"dfp_cia_aberta_2021.zip\" -> extrai \"2021\"\n",
    "            year = int(nome_arquivo.replace(\"dfp_cia_aberta_\", \"\").replace(\".zip\", \"\"))\n",
    "        except:\n",
    "            continue\n",
    "        if year not in anos_processar:\n",
    "            anos_processar[year] = {\"ITR\": None, \"DFP\": None}\n",
    "        anos_processar[year][\"DFP\"] = data_mod\n",
    "    # Filtra somente range 2011 ~ ano atual\n",
    "    ano_atual = datetime.datetime.now().year\n",
    "    anos_filtrados = {}\n",
    "    for y, info in anos_processar.items():\n",
    "        if 2011 <= y <= ano_atual + 1:\n",
    "            anos_filtrados[y] = info\n",
    "    return anos_filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2021: {'ITR': '06-Apr-2025', 'DFP': '06-Apr-2025'},\n",
       " 2022: {'ITR': '06-Apr-2025', 'DFP': '06-Apr-2025'},\n",
       " 2023: {'ITR': '06-Apr-2025', 'DFP': '06-Apr-2025'},\n",
       " 2024: {'ITR': '06-Apr-2025', 'DFP': '06-Apr-2025'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_buscar_arquivos_modificados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dfp_bpa, df_dfp_bpp, df_dfp_dre, df_dfp_dfc_mi = get_dfp_data_by_year(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30798 entries, 0 to 30797\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   CNPJ_CIA       30798 non-null  object \n",
      " 1   DT_REFER       30798 non-null  object \n",
      " 2   VERSAO         30798 non-null  int64  \n",
      " 3   DENOM_CIA      30798 non-null  object \n",
      " 4   CD_CVM         30798 non-null  int64  \n",
      " 5   GRUPO_DFP      30798 non-null  object \n",
      " 6   MOEDA          30798 non-null  object \n",
      " 7   ESCALA_MOEDA   30798 non-null  object \n",
      " 8   ORDEM_EXERC    30798 non-null  object \n",
      " 9   DT_INI_EXERC   30798 non-null  object \n",
      " 10  DT_FIM_EXERC   30798 non-null  object \n",
      " 11  CD_CONTA       30798 non-null  object \n",
      " 12  DS_CONTA       30798 non-null  object \n",
      " 13  VL_CONTA       30798 non-null  float64\n",
      " 14  ST_CONTA_FIXA  30798 non-null  object \n",
      "dtypes: float64(1), int64(2), object(12)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_dfp_dre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'insert_data_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minsert_data_v2\u001b[49m(df_dfp_bpa)\n\u001b[0;32m      2\u001b[0m insert_data_v2(df_dfp_bpp)\n\u001b[0;32m      3\u001b[0m insert_data_v2(df_dfp_dre)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'insert_data_v2' is not defined"
     ]
    }
   ],
   "source": [
    "insert_data_v2(df_dfp_bpa)\n",
    "insert_data_v2(df_dfp_bpp)\n",
    "insert_data_v2(df_dfp_dre)\n",
    "insert_data_v2(df_dfp_dfc_mi)\n",
    "insert_data_v2(df_itr_bpa)\n",
    "insert_data_v2(df_itr_bpp)\n",
    "insert_data_v2(df_itr_dre)\n",
    "insert_data_v2(df_itr_dfc_mi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
